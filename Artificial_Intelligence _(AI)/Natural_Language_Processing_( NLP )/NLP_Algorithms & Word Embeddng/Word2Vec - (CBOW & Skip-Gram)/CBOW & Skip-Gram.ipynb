{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1322ee8f-94d1-4aa6-973f-2ffa974ac0d0",
   "metadata": {},
   "source": [
    "# Word2Vec: CBOW & Skip-Gram\n",
    "\n",
    "- CBOW (Continuous Bag of Words) → Predict target word from context words.\n",
    "- Skip-Gram → Predict context words from target word.\n",
    "\n",
    "##CBOW vs Skip-Gram Explained\n",
    "\n",
    "**1.CBOW (Continuous Bag of Words)**\n",
    "\n",
    " - Goal: Predict the **target word** based on its surrounding **context words**.\n",
    " - Example: Sentence → \"I live in India\"\n",
    "     - Context = [\"I\", \"in\", \"India\"]\n",
    "     - Target = \"live\"\n",
    "- The model tries to guess “live” from the given context words.\n",
    "- Works well for frequent words, and training is fast.\n",
    "\n",
    "**2. Skip-Gram**\n",
    "\n",
    "- Goal: Predict the **context words** given the **target word**.\n",
    "- Example: Sentence → \"I live in India\"\n",
    "    - Target = \"live\"\n",
    "    - Context = [\"I\", \"in\", \"India\"]\n",
    "- The model tries to guess [\"I\", \"in\", \"India\"] from the word “live”.\n",
    "- Better for rare words, but training is slower.\n",
    "\n",
    "**3. Key Difference**\n",
    "\n",
    "- CBOW → context → target word\n",
    "- Skip-Gram → target word → context\n",
    "\n",
    "**4. Intuition**\n",
    "\n",
    "- CBOW is like: “Given the surrounding words, what word fits in the blank?”\n",
    "- Skip-Gram is like: “Given this word, what are the nearby words?”\n",
    "\n",
    "**5.In Practice (Word2Vec)**\n",
    "\n",
    "- Both CBOW and Skip-Gram learn **word embeddings** (dense vectors).\n",
    "- You can switch between them in Gensim with sg=0 (CBOW) and sg=1 (Skip-Gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50d32c1-c045-4003-9d5d-304a09fe044f",
   "metadata": {},
   "source": [
    "# Corpus\n",
    " - We'll use a small text corpus for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749dcd45-76fd-4de7-b665-44985855f2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [\"india\", \"is\", \"a\", \"country\"],\n",
    "    [\"delhi\", \"is\", \"the\", \"capital\", \"of\", \"india\"],\n",
    "    [\"fruits\", \"like\", \"apple\", \"and\", \"mango\"],\n",
    "    [\"england\", \"and\", \"uk\", \"are\", \"related\"],\n",
    "    [\"bharat\", \"is\", \"another\", \"name\", \"for\", \"india\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfab3015-7a53-4c94-9fb0-b51d1892676f",
   "metadata": {},
   "source": [
    "# Train CBOW Model\n",
    "\n",
    "CBOW → Input = context words, Output = target word.\n",
    "Faster, works better with frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8182427a-02df-4920-92d8-7aae53b3ecbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'india' (CBOW):\n",
      " [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n",
      "\n",
      "Most similar to 'india' (CBOW): [('bharat', 0.2705654501914978), ('england', 0.2105751484632492), ('for', 0.16704076528549194), ('mango', 0.15019892156124115), ('the', 0.13204392790794373), ('and', 0.1267007291316986), ('of', 0.0998455360531807), ('apple', 0.0706452950835228), ('are', 0.059369925409555435), ('related', 0.04979120194911957)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# CBOW (sg=0)\n",
    "cbow_model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=0)\n",
    "\n",
    "print(\"Vector for 'india' (CBOW):\\n\", cbow_model.wv[\"india\"])\n",
    "print(\"\\nMost similar to 'india' (CBOW):\", cbow_model.wv.most_similar(\"india\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fdb94b-9273-43e1-abbe-68741185dfe4",
   "metadata": {},
   "source": [
    "# Train Skip-Gram Model\n",
    "Skip-Gram → Input = target word, Output = context words.\n",
    "Slower, but captures rare words better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6303dcaf-0c96-4114-8226-28668b50de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'india' (Skip-Gram):\n",
      " [-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n",
      "\n",
      "Most similar to 'india' (Skip-Gram): [('bharat', 0.2705654501914978), ('england', 0.21058684587478638), ('for', 0.16704076528549194), ('mango', 0.15020102262496948), ('the', 0.13204392790794373), ('and', 0.1267007440328598), ('of', 0.0998455360531807), ('apple', 0.07064357399940491), ('are', 0.05936861038208008), ('related', 0.04979120194911957)]\n"
     ]
    }
   ],
   "source": [
    "# Skip-Gram (sg=1)\n",
    "skipgram_model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=1)\n",
    "\n",
    "print(\"Vector for 'india' (Skip-Gram):\\n\", skipgram_model.wv[\"india\"])\n",
    "print(\"\\nMost similar to 'india' (Skip-Gram):\", skipgram_model.wv.most_similar(\"india\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731311c-ad37-45e0-b08b-80e24c5902ab",
   "metadata": {},
   "source": [
    "# Key Insights\n",
    "- CBOW → Predicts target from context, faster, frequent words.\n",
    "- Skip-Gram → Predicts context from target, better for rare words.\n",
    "- Both create **word embeddings** that capture meaning.\n",
    "# Real-World Applications:\n",
    "- CBOW → Large datasets (e.g., News, Wikipedia).\n",
    "- Skip-Gram → Domain-specific tasks (e.g., Medical, Legal).\n",
    "- Used in **Search engines, Chatbots, Recommendation systems, LLMs**.\n",
    "- \n",
    "# NLP Fundamentals\n",
    "Over the past few sessions, we explored the foundations of **Natural Language Processing (NLP)**.\n",
    "Here’s the journey in short:\n",
    "\n",
    "**Text Preprocessing**\n",
    "- Tokenization, Stopwords removal, Lemmatization, Stemming.\n",
    "- Converting raw text into a clean format for ML/NLP tasks.\n",
    "**Text Representation**\n",
    "- Bag of Words (BoW).\n",
    "- TF-IDF (Term Frequency–Inverse Document Frequency).\n",
    "- Learned how frequency-based methods convert text into numerical vectors.\n",
    "**Word Embeddings Basics**\n",
    "- Difference between One-Hot Encoding & Embeddings.\n",
    "- Why embeddings capture semantic meaning while dummy variables don’t.\n",
    "**Word2Vec**\n",
    "- Trained Word2Vec models on small corpora.\n",
    "- Understood context-based word similarity (e.g., king – man + woman = queen).\n",
    "- PCA visualization of embeddings.\n",
    "**BoW vs TF-IDF vs Word2Vec**\n",
    "- Compared frequency-based and context-based methods.\n",
    "- Highlighted how Word2Vec learns meaning instead of just counting words.\n",
    "**CBOW & Skip-Gram**\n",
    "- CBOW (Continuous Bag of Words): Predicts target word from context → faster, better for frequent words.\n",
    "- Skip-Gram: Predicts context from target → slower, better for rare words.\n",
    "- Visualized embeddings with PCA.\n",
    "\n",
    "##Key Takeaways\n",
    "\n",
    "- BoW → Simple, but ignores meaning & order.\n",
    "- TF-IDF → Better, highlights important words.\n",
    "- Word2Vec → Learns true semantic relationships.\n",
    "- CBOW vs Skip-Gram → Tradeoff between speed & rare-word performance.\n",
    "\n",
    "##Real-World Applications\n",
    "\n",
    "- Text Classification (spam detection, sentiment analysis).\n",
    "- Semantic Search & Information Retrieval.\n",
    "- Chatbots & Virtual Assistants.\n",
    "- Recommendation Systems.\n",
    "- Foundation for **LLMs (Large Language Models).**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
